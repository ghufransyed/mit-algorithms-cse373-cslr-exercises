3.1-1
let f(n) and g(n) be asymptotically non-negative functions

Prove that max(f(n),g(n)) = theta(f(n) + g(n))

0 <= f(n) <= max(f(n),g(n)) <= f(n) + g(n)
0 <= g(n) <= max(f(n),g(n)) <= f(n) + g(n)
0 <= f(n) + g(n) <= 2max(f(n),g(n)), so
0 <= 1/2(f(n) + g(n)) <= max(f(n),g(n)) <= f(n) + g(n)

let h(n) = (f(n) + g(n)). Then
0 <= 1/2h(n) <= max(f(n),g(n)) <= h(n)
so
max(f(n),g(n)) = theta(h(n))

3.1-2

Prove that (n+a)^b = theta(n^b)

n-|a| <= n+a <= n+|a|
When |a|<=n, then
n+a <= 2n

When |a|<=(n/2), then
n/2 <= n-|a|

So when |a| <= (n/2) ,
n >= |2a| >= |a|, and 
0 <= n/2 <= n-|a| <= n+a <= n+|a| <= 2n
0 <= n/2 <= n+a <= 2n

Since b > 0,
0^b <= (n/2)^b <= (n+a)^b <= (2n)^b
0   <= (1/2)^b * n^b <= (n+a)^b <= 2^b * n^b

since (1/2)^b and (2)^b are constants, 
(n+a)^b = theta(n^b)


3.1-3
Explain why the statement,
"The running time of Algorithm A is at least O(n^2)" is meaningless.

Let the running time be T(n). T(n) >= O(n^2) means that
T(n) >= f(n) for some function in the set O(n^2)

However, the set O(n^2) means the set of functions f(n),
there exist constants n_0 and c, such that
0 <= f(n) <= c*g(n) for all n > n_0

However, the function h(n)=0 for all n is in O(n^2), and 
running times are always non-negative. So stating that 
T(n) >= O(n^2) is equivalent to stating that T(n) >= 0


3.1-4
2^(n+1) = 2*2^n = O(2^n)

2^(2n) = (2^2)^n = 4^n = O(4^n) != O(2^n)

3.1-5

let P(n): f(n)= theta(g(n))
let Q(n): f(n) = O(g(n)) and f(n) = Omega(g(n))

Prove P -> Q:
Assume P
Then 0 <= c1*g(n) <= f(n) <= c2*g(n)
So 0<= c1*g(n) <= f(n) , meaning f(n) = Omega(g(n))
And 0 <= f(n) <= c2*g(n), meaning f(n) = O(g(n))

Prove Q -> P:
Assume Q
Then 0 <= c1*g(n) <= f(n) and
0 <= f(n) <= c2*g(n)
so 0<= c1*g(n) <= f(n) <= c2*g(n), so f(n) = theta(g(n))
QED


3.1-6
Prove that the running time of an algorithm is theta(g(n))
if and only if its worst-case running time is O(g(n))
and its best-case running time is Omega(g())

Let P(n): f(n) = theta(g(n))

Let Tb(n) = best-case running time
Let Tw(n) = worst-case running time
Let Q(n): Tb(n) = Omega(g(n)), Tw(n) = O(g(n))

Proof:
P -> Q

Assume P(n)
Then 0 <= c1*g(n) <= f(n) <= c2*g(n)
Tb(n) <= f(n) <= Tw(n)
Then 0 <= c1*g(n)<=Tb(n)
i.e. the best-case running time is Omega(g(n))

Similarly,
0 <= Tw(n) <= c2*g(n), so
the worst-case running time is O(g(n))

Q -> P
Assume Q(n)
This proof is essentially P -> Q in reverse:
If Q(n), then  Tb(n) = Omega(g(n)), Tw(n) = O(g(n))
Then 0 <= c1*g(n)<=Tb(n)
and 0 <= Tw(n) <= c2*g(n)
Since Tb(n) <= f(n) <= Tw(n),
0 <= c1*g(n) <= Tb(n) <= f(n) <=Tw(n) <= c2*g(n)
so f(n) = theta(g(n))


3.1-8
O(g(n,m)) = {f(n,m): there exist positive constants c,n_0,m_0
    such that 0 <= f(n,m) <= cg(n,m)
    for all n >= n_0 or m >= m_0 }

Omega(g(n,m)) = {f(n,m): there exist positive constants c,n_0,m_0
    such that 0 <= cg(n,m) <= f(n,m)
    for all n >= n_0 or m >= m_0 }

Theta(g(n,m)) = {f(n,m): there exist positive constants c1,c2 ,n_0,m_0
    such that 0 <= c1g(n,m) <= f(n,m) <= c2g(n,m)
    for all n >= n_0 or m >= m_0 }


3.2-1

Show that if f(n) and g(n) are monotonically increasing functions,
then so are the functions f(n)+g(n) and f(g(n)),
and if f(n) and g(n) are in addition nonnegative,
then f(n)*g(n) is monotonically increasing.


3.2-2
Prove that a^log_b(c) = c^log_b(a)

log_a(c) = log_b(c)/log_b(a)
log_b(c) = log_a(c) * log_b(a)
log_a(a) * log_b(c) = log_a(c) * log_b(a)
log_a(a)^log_b(c) = log_a(c)^log_b(a)
a^log_b(c) = c^log_b(a)

QED


3.2-3
Prove that lg(n!) = theta(nlg(n))

Using Stirling's approximation:
lg(n!) = lg(sqrt(2*pi*n)(n/e)^n(1+theta(1/n)))
= lg(sqrt(2*pi*n)) + lg(n/e)^n + lg(1+theta(1/n))
= lg(sqrt(2*pi)) + lg(sqrt(n)) + n*lg(n/e) + lg(1+theta(1/n))
= theta(1) + 1/2lg(n) + theta(n*lg_n) + lg(theta(1) + theta(1/n))
= theta(1) + theta(lg(n)) + theta(n*lg(n)) + theta(lg(1/n))
= theta(n*lg(n))

2^n = 2 * 2 * 2 ... n times
n!  = 1 * 2 * 3 ... n
So for all n > 2, 2^n < n! so n! = little_omega(2^n)

n^n = n * n * n * ..* n
n!  = 1 * 2 * 3 * ..* n

So for all n > 1, n! < n^n so n! = o(n^n)

